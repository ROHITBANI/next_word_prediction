{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 581888\n",
      "Number of words: 107603\n",
      "First 10 words: ['\\ufeff', 'project', \"gutenberg's\", 'the', 'adventures', 'of', 'sherlock', 'holmes,', 'by', 'arthur']\n"
     ]
    }
   ],
   "source": [
    "path = 'book.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "# Split the text into individual words\n",
    "words = text.split()\n",
    "print('Number of words:', len(words))\n",
    "print('First 10 words:', words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('the', 5703), ('and', 2882), ('of', 2758), ('to', 2720), ('a', 2648), ('i', 2533), ('in', 1760), ('that', 1605), ('was', 1371), ('he', 1278)]\n",
      "Number of least common words: 8576\n",
      "Most common bigrams: [(('of', 'the'), 740), (('in', 'the'), 511), (('to', 'the'), 313), (('i', 'have'), 247), (('that', 'i'), 245), (('it', 'was'), 244), (('at', 'the'), 238), (('it', 'is'), 235), (('upon', 'the'), 196), (('and', 'the'), 193)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "most_common_words = word_counts.most_common(10)  # Top 10 most frequent words\n",
    "least_common_words = [word for word, count in word_counts.items() if count == 1]  # Words that appear only once\n",
    "\n",
    "print('Most common words:', most_common_words)\n",
    "print('Number of least common words:', len(least_common_words))\n",
    "\n",
    "# Generate bigrams (pairs of consecutive words)\n",
    "word_bigrams = list(bigrams(words))\n",
    "bigram_counts = FreqDist(word_bigrams)\n",
    "most_common_bigrams = bigram_counts.most_common(10)  # Top 10 most frequent bigrams\n",
    "\n",
    "print('Most common bigrams:', most_common_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohit\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 107598\n",
      "First 5 sequences and their next words:\n",
      "Sequence: ['\\ufeff', 'project', \"gutenberg's\", 'the', 'adventures'] -> Next word: of\n",
      "Sequence: ['project', \"gutenberg's\", 'the', 'adventures', 'of'] -> Next word: sherlock\n",
      "Sequence: [\"gutenberg's\", 'the', 'adventures', 'of', 'sherlock'] -> Next word: holmes,\n",
      "Sequence: ['the', 'adventures', 'of', 'sherlock', 'holmes,'] -> Next word: by\n",
      "Sequence: ['adventures', 'of', 'sherlock', 'holmes,', 'by'] -> Next word: arthur\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 5  # Set the length of input sequences\n",
    "sequences = []\n",
    "next_words = []\n",
    "\n",
    "# Generate sequences of 5 consecutive words and their next word\n",
    "for i in range(len(words) - sequence_length):\n",
    "    sequences.append(words[i:i + sequence_length])  # Input: X consecutive words\n",
    "    next_words.append(words[i + sequence_length])  # Output: The next word\n",
    "\n",
    "print(f'Number of sequences: {len(sequences)}')\n",
    "print('First 5 sequences and their next words:')\n",
    "for seq, next_word in zip(sequences[:5], next_words[:5]):\n",
    "    print(f'Sequence: {seq} -> Next word: {next_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary of unique words and add <UNK> for unknown words\n",
    "vocabulary = sorted(set(words))\n",
    "vocabulary.append('<UNK>')  # Add <UNK> token\n",
    "vocab_size = len(vocabulary)\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "index_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Encode sequences and next words as indices\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(words) - sequence_length):\n",
    "    sequence = words[i:i + sequence_length]\n",
    "    next_word = words[i + sequence_length]\n",
    "    X.append([word_to_index[word] for word in sequence])\n",
    "    y.append(word_to_index[next_word])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# One-hot encode the output\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Split the data into training, testing, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 32ms/step - accuracy: 0.0522 - loss: 7.4231 - val_accuracy: 0.0529 - val_loss: 6.9141\n",
      "Epoch 2/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 31ms/step - accuracy: 0.0653 - loss: 6.6027 - val_accuracy: 0.0980 - val_loss: 6.6850\n",
      "Epoch 3/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 31ms/step - accuracy: 0.1056 - loss: 6.0480 - val_accuracy: 0.1100 - val_loss: 6.7165\n",
      "Epoch 4/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.1293 - loss: 5.6634 - val_accuracy: 0.1160 - val_loss: 6.7891\n",
      "Epoch 5/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 37ms/step - accuracy: 0.1516 - loss: 5.3177 - val_accuracy: 0.1231 - val_loss: 6.9625\n",
      "Epoch 6/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 37ms/step - accuracy: 0.1773 - loss: 4.9957 - val_accuracy: 0.1223 - val_loss: 7.3279\n",
      "Epoch 7/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 37ms/step - accuracy: 0.1988 - loss: 4.6860 - val_accuracy: 0.1240 - val_loss: 7.5897\n",
      "Epoch 8/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 185ms/step - accuracy: 0.2256 - loss: 4.3857 - val_accuracy: 0.1201 - val_loss: 8.1437\n",
      "Epoch 9/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 32ms/step - accuracy: 0.2504 - loss: 4.0808 - val_accuracy: 0.1172 - val_loss: 8.7153\n",
      "Epoch 10/10\n",
      "\u001b[1m1345/1345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 32ms/step - accuracy: 0.2776 - loss: 3.7715 - val_accuracy: 0.1067 - val_loss: 9.5140\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.1078 - loss: 9.8278\n",
      "Test Loss: 9.701238632202148\n",
      "Test Accuracy: 0.10752788186073303\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a vocabulary of unique words and add <UNK> for unknown words\n",
    "vocabulary = sorted(set(words))\n",
    "vocabulary.append('<UNK>')  # Add <UNK> token\n",
    "vocab_size = len(vocabulary)\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "index_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Encode sequences and next words as indices\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(words) - sequence_length):\n",
    "    sequence = words[i:i + sequence_length]\n",
    "    next_word = words[i + sequence_length]\n",
    "    X.append([word_to_index[word] for word in sequence])\n",
    "    y.append(word_to_index[next_word])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# One-hot encode the output\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Split the data into training, testing, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "# Build a simple neural network model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, input_length=sequence_length),  # Embedding layer\n",
    "    LSTM(128, return_sequences=False),  # LSTM layer\n",
    "    Dense(128, activation='relu'),  # Fully connected layer\n",
    "    Dense(vocab_size, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,  # Start with 10 epochs; you can increase this later\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'the quick brown fox jumps'\n",
      "Predicted next word: '“did'\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(model, input_text, word_to_index, index_to_word, sequence_length):\n",
    "    \"\"\"\n",
    "    Predict the next word given an input sentence of X words.\n",
    "    Handles unknown words by replacing them with <UNK>.\n",
    "    \"\"\"\n",
    "    # Preprocess the input text\n",
    "    input_words = input_text.lower().split()\n",
    "    if len(input_words) != sequence_length:\n",
    "        raise ValueError(f\"Input must contain exactly {sequence_length} words.\")\n",
    "    \n",
    "    # Replace unknown words with <UNK>\n",
    "    input_indices = [\n",
    "        word_to_index[word] if word in word_to_index else word_to_index['<UNK>']\n",
    "        for word in input_words\n",
    "    ]\n",
    "    \n",
    "    # Reshape input for prediction\n",
    "    input_array = np.array(input_indices).reshape(1, -1)\n",
    "    \n",
    "    # Predict the next word\n",
    "    predictions = model.predict(input_array, verbose=0)\n",
    "    predicted_index = np.argmax(predictions)\n",
    "    predicted_word = index_to_word[predicted_index]\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Example usage\n",
    "custom_input = \"the quick brown fox jumps\"  # Replace with your own 5-word sentence\n",
    "try:\n",
    "    next_word = predict_next_word(model, custom_input, word_to_index, index_to_word, sequence_length)\n",
    "    print(f\"Input: '{custom_input}'\")\n",
    "    print(f\"Predicted next word: '{next_word}'\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
